\section{SecondBrain Reactor}\label{secondbrain-reactor}

\subsection{The problem is not ideas. It is
visibility.}\label{the-problem-is-not-ideas.-it-is-visibility.}

Every lab meeting, we discuss many promising directions. How should we
focus our attention and follow through? Based on what evidence? Most of
those directions become hard to track, compare, and act on without a
system to organize them.

When I joined the lab, it took me three weeks just to understand what
each team member was working on and where the group was heading. That
context lived in people's heads -- in scattered conversations, slides,
in the PI's intuition. There was no single place to look. A postdoc
reads a paper that contradicts an assumption but files it under
``interesting'' and moves on. A technician spends long hours at the
bench and is short on time to develop novel ideas. These insights are
real, but without a system to persist them, they fade.

The ideas that do surface face a harder problem: our lab has finite
bench hours, finite sequencing budget, finite analyst bandwidth. Ten
good ideas compete for three experimental slots. The ones that get
pursued are not always the ones best supported by evidence -- they are
the ones someone had time to champion.

The gap is not analytical skill or scientific creativity. It is the
infrastructure to accumulate evidence and convert it into prioritized
action.

\subsection{Two capabilities make this possible, and both exist
today.}\label{two-capabilities-make-this-possible-and-both-exist-today.}

The first is
\textbf{\href{https://github.com/agenticnotetaking/arscontexta}{Ars
Contexta}} -- an open-source knowledge architecture that converts raw
research input into a persistent, queryable graph of atomic claims.
Every observation, literature finding, and experimental result becomes a
structured note with typed links to related claims. The graph is the
lab's institutional memory: browsable, searchable, and derived from the
lab's own research domains.

The second is the \textbf{co-scientist} -- a hypothesis engine that
operates on top of that graph. Inspired by
\href{https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/}{DeepMind's
AI co-scientist} architecture, it runs a competitive loop: generate
hypotheses from the evidence base, debate them pairwise on scientific
merit, rank them by Elo rating, and evolve the strongest through
meta-review feedback. Each cycle sharpens the next without model
retraining.

Neither layer is useful alone at scale. A knowledge graph without
hypothesis generation is a well-organized archive. A hypothesis engine
without structured evidence is speculation. The combination is what
shifts the bottleneck from human attention to evidence quality.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{System architecture}\label{system-architecture}

EngramR combines both layers into a single system. Here is how each works in
practice.

\subsubsection{Ars Contexta -- the knowledge
layer}\label{ars-contexta-the-knowledge-layer}

When a lab sets up EngramR, Ars Contexta derives the knowledge system
-- folder structure, topic maps, visualization standards -- from a
conversation about the lab's research domains and working style. The result is a vault: a structured folder of linked
notes that any team member can browse.

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.1484}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.8516}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What it does
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Atomic claims} & Each insight becomes a single note with
structured metadata and typed links to related claims. \\
\textbf{Knowledge graph} & Claims link to each other through wiki-link
edges. Topic maps organize clusters into navigable neighborhoods. \\
\end{longtable}
}

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.1257}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.8743}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Development
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
How it works
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Processing pipeline} & Raw input (papers, observations,
datasets) enters an inbox and passes through extraction,
connection-finding, and quality verification before joining the
graph. \\
\textbf{Consistent outputs} & Templates enforce the same reporting
format, figure style, and statistical conventions across every team
member and every analysis. \\
\end{longtable}
}

\subsubsection{Co-scientist -- the hypothesis
layer}\label{co-scientist-the-hypothesis-layer}

The competitive loop runs five stages:

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.0659}}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.9341}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What it does
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Generate} & When related claims cluster, the system proposes
testable hypotheses -- each with mechanism, predictions, analysis plan,
and falsification criteria. \\
\textbf{Debate} & Hypotheses compete pairwise in structured scientific
debates evaluating novelty, plausibility, testability, and impact. Every
debate produces a transcript. \\
\textbf{Rank} & Elo ratings update after each debate. The leaderboard
reflects cumulative outcomes across all rounds. \\
\textbf{Evolve} & Meta-reviews synthesize what made winners win and
losers lose. That feedback injects into the next generation. Hypotheses
sharpen across cycles without model fine-tuning. \\
\textbf{Execute} & Top hypotheses come with pre-specified analysis plans
and statistical tests. When a user green-lights, the analysis is ready
to run. \\
\end{longtable}
}

\textbf{Interface.} The primary interface is Slack -- send observations,
ask questions, get notifications, check the leaderboard. The vault is
available for deeper exploration when needed.

\subsubsection{Goals, hypotheses, and
projects}\label{goals-hypotheses-and-projects}

The system organizes work at three levels: goals, hypotheses, and
projects.

{\def\LTcaptype{none} % do not increment counter
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.0915}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3293}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3171}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2622}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Goal
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Hypothesis
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Project
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Definition} & Open-ended research question & Testable prediction
with mechanism & Concrete work with defined scope \\
\textbf{Example} & ``Identify biomarker signatures for early AD
detection'' & ``Ceramide-plasmalogen ratio predicts amyloid status'' &
``Run the ratio analysis on ADNI cohort'' \\
\textbf{EngramR role} & Defines the research direction & Competes for rank
under a goal & Executes and feeds results back \\
\textbf{Lifecycle} & Evolves as the lab learns & Generated, debated,
ranked, evolved & Has timeline, budget, deliverables \\
\textbf{Elo applies} & Organizes the leaderboard & Yes -- hypotheses
compete head-to-head & No -- projects execute, they do not compete \\
\end{longtable}
}

Goals spawn hypotheses. Hypotheses spawn projects. Project results feed
back into the hypothesis pool, reshaping the leaderboard.

\subsubsection{The reactor scales with
use}\label{the-reactor-scales-with-use}

EngramR starts on a blank slate, without any migrations necessary. It runs
alongside whatever the team already uses.

The more the team contributes, the more context the reactor has to work
with -- and the sharper its outputs become. Early on, the system
captures and connects. Then, it begins generating hypotheses. After some
time, the knowledge graph is dense enough that new observations land in
a web of existing connections.

Teams can register their own datasets -- sequencing runs, assay results,
imaging archives -- alongside public datasets available to them (ADNI,
ROSMAP, UK Biobank, GEO). When the reactor generates a hypothesis, it
checks what data can actually test it. Hypotheses testable with data
already on hand rank differently from those that require a new cohort or
a grant cycle. Data availability becomes an active input that shapes
which directions the system prioritizes.

Adoption is not a cliff. It is a gradient. Send one Slack message and
you have contributed. Browse the leaderboard and you understand the
lab's priorities. Run an analysis and feed back your observations. The
deeper the team engages, the more the reactor gives back.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{EngramR in use}\label{sbr-in-use}

\subsubsection{From an observation to
results}\label{from-an-observation-to-results}

\textbf{9:02am} -- The tech notices something unexpected in her
experiment. She sends a Slack message.

\begin{quote}
``Seeing unexpected protein accumulation in the treated group at 24h.
Wasn't part of the original hypothesis.''
\end{quote}

She goes back to her bench.

\textbf{9:03am} -- The reactor extracts a structured claim, tags it, and
searches the knowledge graph. Three connections surface: a grad
student's sequencing data from March that showed the same pathway
activating, a paper the postdoc read last week about a related
mechanism, and an existing hypothesis (H-012, Elo 1195) that predicted
this involvement -- but through a different mechanism. The claim is
linked to all three. A tension is flagged: the data supports the
prediction but not the proposed mechanism.

\textbf{9:40am} -- The postdoc gets a Slack notification: a new
observation was linked to a literature note he submitted last week. He
did not know anyone in the lab was generating data in this area. He taps
through, reads the tension flag, and sends a follow-up.

\begin{quote}
``Her data fits the alternative pathway better. If that's the mechanism,
we should see it in the proteomics dataset we already have.''
\end{quote}

\textbf{9:41am} -- The reactor links the postdoc's claim to the growing
cluster. Evidence density crosses the threshold. The system generates an
evolved version of H-012 -- same core prediction, updated mechanism
based on the converging evidence. The evolved hypothesis comes with a
pre-specified analysis plan and statistical tests.

\textbf{9:45am} -- The evolved hypothesis enters the tournament and
debates the original H-012 head-to-head on specificity, evidence
support, and testability. It wins -- three independent data sources
where the original had one. Elo updates. Debate transcript stored.

\textbf{11:15am} -- The postdoc executes the analysis. He does not start
from scratch -- the hypothesis note contains the test, the parameters,
and the figure template. Results: two of three predictions confirmed.
One is inconclusive. The system logs the experiment, links results to
predictions, flags the gap for the next debate cycle.

\textbf{11:30am} -- The graph updates. The hypothesis holds its rank but
carries a documented weakness. Next round, that weakness is fair game.

One morning. Two people sent messages while doing their actual work. The
reactor connected their observations, evolved a hypothesis, debated it,
ranked it, and pre-specified the analysis. By late morning, results were
in and feeding back into the system. Nobody stopped what they were
doing.

\subsubsection{From anomaly to project
proposal}\label{from-anomaly-to-project-proposal}

A grad student uploads her dataset to EngramR for exploratory analysis. The
system generates a report -- plots in the lab's standard style, summary
statistics, all processed locally on the lab's own infrastructure. One
finding stands out: a pattern in her data that does not match any
existing hypothesis in the graph.

She asks: ``What could explain this?''

The reactor searches the knowledge graph. It finds six related claims
from literature and prior observations, identifies two possible
mechanisms, and generates a hypothesis from the anomaly -- complete with
predictions, a validation plan using data already on the server, and
falsification criteria. The hypothesis enters the tournament that week.

The anomaly that would have been a footnote in her thesis is now a
ranked project proposal with structured evidence behind it.

\subsubsection{Administration}\label{administration}

\textbf{Writing aims from evidence.} The PI needs specific aims for an
R01. She defines a research goal in EngramR. The system generates six
hypotheses from the existing knowledge graph, she runs a tournament to
rank them. The top three become her aims -- each with mechanism,
predictions, and preliminary data already identified from the lab's own
datasets. The debate transcripts document why these beat the
alternatives.

\textbf{Allocating resources.} The PI has budget for one new project.
She filters the leaderboard by hypotheses testable with existing data.
Three candidates surface with different cost profiles: one needs only a
proteomics dataset from January, another needs RNA-seq already on the
server, a third would require a new cohort. The decision takes minutes.
The evidence trail is there for anyone to review.

\textbf{Opening a new direction.} The PI reads a paper that opens an
unexplored direction. She defines a new goal. The system scans the
existing graph -- twelve claims are already relevant, two projects have
transferable data. The reactor generates three seed hypotheses from what
the lab already knows. The new direction starts with context, not from
zero.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Scaling across labs}\label{scaling-across-labs}

One reactor per lab is already valuable. Multiple reactors become
something qualitatively different.

Two labs working in adjacent areas -- say, neuroinflammation and
metabolomics -- each run their own EngramR instance. Each generates and
ranks hypotheses against their own data. But the knowledge graphs can be
selectively bridged. Lab A's observation about microglial lipid handling
connects to Lab B's finding about ceramide ratios in patient plasma.
Neither lab would have made that connection alone.

Cross-lab EngramR does not require merging teams or sharing raw data. It
requires sharing claims -- structured, atomic, properly attributed
observations that link across knowledge graphs. Elo tournaments can
include cross-lab hypotheses, letting ideas from different groups
compete on equal footing.

This inverts the traditional collaboration model. Instead of two PIs
deciding to collaborate and then looking for scientific overlap, the
system surfaces the overlap first. The knowledge graph identifies the
bridges. The PIs decide whether to cross them.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

The reactor is running. The question is which ideas to put into it.
